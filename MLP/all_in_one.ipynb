{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212814bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#multilayer perceptron- N binary inputs, two hidden layers and one binary output\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n = int(input(\"Enter the number of inputs: \"))\n",
    "total_cases = 2**n\n",
    "#generate all combinations of inputs\n",
    "inputs = np.array([[int(x) for x in format(i,f'0{n}b')] for i in range(total_cases)])\n",
    "\n",
    "expected_outputs = []\n",
    "print(\"\\nEnter the expected output for each combination:\")\n",
    "for row in inputs:\n",
    "    output = int(input(f\"Input: {row} -\"))\n",
    "    expected_outputs.append(output)\n",
    "expected_outputs = np.array(expected_outputs)\n",
    "\n",
    "# for 2 outputs\n",
    "# expected_outputs = []\n",
    "# print(\"\\nEnter the expected 2 outputs for each combination (separated by space):\")\n",
    "# for row in inputs:\n",
    "#     output = list(map(int, input(f\"Input: {row} - \").split()))\n",
    "#     expected_outputs.append(output)\n",
    "# expected_outputs = np.array(expected_outputs)\n",
    "\n",
    "#bias entered for each layer\n",
    "bias1 = float(input(\"\\nEnter bias1: \")) #Hidden Layer 1\n",
    "bias2 = float(input(\"Enter bias2: \")) #Hidden Layer 2 \n",
    "bias3 = float(input(\"Enter bias3: \")) #Output Layer \n",
    "\n",
    "#hidden layer sizes\n",
    "hidden1_neurons = n\n",
    "hidden2_neurons = n\n",
    "\n",
    "\n",
    "training_step = 0\n",
    "while True:\n",
    "    training_step += 1\n",
    "    found = True\n",
    "    outputs = []\n",
    "\n",
    "    w1 = np.round(np.random.uniform(-1.5, 1.5, (n,hidden1_neurons)),4)\n",
    "    w2 = np.round(np.random.uniform(-1.5, 1.5, (hidden1_neurons, hidden2_neurons)), 4)\n",
    "    w3 = np.round(np.random.uniform(-1.5, 1.5, hidden2_neurons), 4)\n",
    "\n",
    "    for i in range(total_cases):\n",
    "        net1 = np.dot(inputs[i],w1)+bias1\n",
    "        net2 = np.dot(net1,w2)+bias2\n",
    "        net3 = np.dot(net2,w3)+bias3\n",
    "\n",
    "        output = 1 if net3>=0 else 0\n",
    "        outputs.append(output)\n",
    "\n",
    "        if output!=expected_outputs[i]:\n",
    "            found= False\n",
    "            break # try again with random weights\n",
    "    if found:\n",
    "        break\n",
    "\n",
    "print(f\"\\nNeural Network with {n} inputs, one output and 2 hidden layers\")\n",
    "print(f\"\\nOutput matched expected results in {training_step} training step(s)\\n\")\n",
    "\n",
    "print(\"Weight 1 matrix :\")\n",
    "print(w1)\n",
    "print(\"\\nWeight 2 matrix:\")\n",
    "print(w2)\n",
    "print(\"\\nWeight 3 matrix:\")\n",
    "print(w3)\n",
    "\n",
    "for i in range(total_cases):\n",
    "    print(f\"Input: {inputs[i]} \\t Expected: {expected_outputs[i]} \\t Output: {outputs[i]}\")\n",
    "\n",
    "----------------------------------------------------------------\n",
    "# Multilayer Perceptron - 4 inputs, 1 hidden layer, 2 outputs\n",
    "import numpy as np\n",
    "\n",
    "n = 4  # fixed 4 inputs\n",
    "total_cases = 2**n\n",
    "\n",
    "# generate all input combinations\n",
    "inputs = np.array([[int(x) for x in format(i,f'04b')] for i in range(total_cases)])\n",
    "\n",
    "expected_outputs = []\n",
    "print(\"\\nEnter the expected 2 outputs for each combination (separated by space):\")\n",
    "for row in inputs:\n",
    "    output = list(map(int, input(f\"Input: {row} - \").split()))\n",
    "    expected_outputs.append(output)\n",
    "expected_outputs = np.array(expected_outputs)\n",
    "\n",
    "# biases for hidden layer and output layer\n",
    "bias1 = float(input(\"\\nEnter bias1 (for hidden layer): \"))\n",
    "bias2 = float(input(\"Enter bias2 (for output layer): \"))\n",
    "\n",
    "# hidden layer size\n",
    "hidden_neurons = 4\n",
    "output_neurons = 2\n",
    "\n",
    "training_step = 0\n",
    "while True:\n",
    "    training_step += 1\n",
    "    found = True\n",
    "    outputs = []\n",
    "\n",
    "    w1 = np.round(np.random.uniform(-1.5, 1.5, (n, hidden_neurons)), 4)  # input -> hidden\n",
    "    w2 = np.round(np.random.uniform(-1.5, 1.5, (hidden_neurons, output_neurons)), 4)  # hidden -> output\n",
    "\n",
    "    for i in range(total_cases):\n",
    "        net1 = np.dot(inputs[i], w1) + bias1\n",
    "        net2 = np.dot(net1, w2) + bias2\n",
    "\n",
    "        output = [1 if net2[j] >= 0 else 0 for j in range(output_neurons)]\n",
    "        outputs.append(output)\n",
    "\n",
    "        if not np.array_equal(output, expected_outputs[i]):\n",
    "            found = False\n",
    "            break  # try again\n",
    "\n",
    "    if found:\n",
    "        break\n",
    "\n",
    "print(f\"\\nNeural Network with {n} inputs, 1 hidden layer, and 2 outputs\")\n",
    "print(f\"\\nOutput matched expected results in {training_step} training step(s)\\n\")\n",
    "\n",
    "print(\"Weight 1 matrix (Input -> Hidden) :\")\n",
    "print(w1)\n",
    "print(\"\\nWeight 2 matrix (Hidden -> Output) :\")\n",
    "print(w2)\n",
    "\n",
    "for i in range(total_cases):\n",
    "    print(f\"Input: {inputs[i]} \\t Expected: {expected_outputs[i]} \\t Output: {outputs[i]}\")\n",
    "\n",
    "\n",
    "-----------------------------------------------------------\n",
    "#SIGMOID,TANH,RELU\n",
    "import numpy as np \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx*(1-sx)\n",
    "    \n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def binary_cross_entropy(y_true,y_pred):\n",
    "    epsilon=1e-8\n",
    "    return -np.mean(y_true*np.log(y_pred+epsilon)+(1-y_true)*np.log((1-y_pred)+epsilon))\n",
    "def generate_binary_inputs(n):\n",
    "    total_cases = 2 ** n\n",
    "    return np.array([[int(x) for x in format(i, f'0{n}b')] for i in range(total_cases)])\n",
    "\n",
    "\n",
    "def train_network(inputs, expected_outputs, hidden1_neurons, hidden2_neurons, lr=0.1, epochs=10000):\n",
    "    n_samples,n_features = inputs.shape\n",
    "\n",
    "    W1 = np.random.randn(n_features, hidden1_neurons)\n",
    "    B1 = np.zeros((1, hidden1_neurons))\n",
    "    W2 = np.random.randn(hidden1_neurons, hidden2_neurons)\n",
    "    B2 = np.zeros((1, hidden2_neurons))\n",
    "    W3 = np.random.randn(hidden2_neurons, 1)\n",
    "    B3 = np.zeros((1, 1))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "         # Forward pass\n",
    "        Z1 = np.dot(inputs, W1) + B1\n",
    "        A1 = sigmoid(Z1) #A1 = tanh(Z1) #A1 = relu(Z1)\n",
    "        Z2 = np.dot(A1, W2) + B2\n",
    "        A2 = sigmoid(Z2) #A2 = tanh(Z2) #A2 = relu(Z2)\n",
    "        Z3 = np.dot(A2, W3) + B3\n",
    "        A3 = sigmoid(Z3) #A3 = sigmoid(Z3) , A3 = sigmoid(Z3)\n",
    "        loss = binary_cross_entropy(expected_outputs, A3)\n",
    "\n",
    "        dZ3 = A3 - expected_outputs #dZ3 is the gradient of the loss with respect to Z3 (raw score before final sigmoid).\n",
    "        dW3 = np.dot(A2.T, dZ3)# how many neurons contributed to error\n",
    "        dB3 = np.sum(dZ3, axis=0, keepdims=True)\n",
    "\n",
    "        dA2 = np.dot(dZ3, W3.T)\n",
    "        dZ2 = dA2 * sigmoid_derivative(Z2) #dZ2 = dA2 * tanh_derivative(Z2) \n",
    "        dW2 = np.dot(A1.T, dZ2)\n",
    "        dB2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "        dA1 = np.dot(dZ2, W2.T)\n",
    "        dZ1 = dA1 * sigmoid_derivative(Z1) #dZ1 = dA1 * tanh_derivative(Z1) \n",
    "        dW1 = np.dot(inputs.T, dZ1) \n",
    "        dB1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "        W3 -= lr * dW3\n",
    "        B3 -= lr * dB3\n",
    "        W2 -= lr * dW2\n",
    "        B2 -= lr * dB2\n",
    "        W1 -= lr * dW1\n",
    "        B1 -= lr * dB1\n",
    "\n",
    "        if epoch % 1000 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch} | Loss: {loss:.6f}\")\n",
    "            \n",
    "    # Final predictions\n",
    "    final_output = (A3 > 0.5).astype(int)\n",
    "    return final_output,W1,W2,W3\n",
    "\n",
    "n = int(input(\"Enter the number of inputs\"))\n",
    "inputs = generate_binary_inputs(n)\n",
    "expected_outputs = []\n",
    "for row in inputs :\n",
    "    output = int(input(f\"Input:{row}-\"))\n",
    "    expected_outputs.append(output)\n",
    "expected_outputs = np.array(expected_outputs).reshape(-1,1)\n",
    "\n",
    "hidden1 = int(input(\"\\nEnter number of neurons in Hidden Layer 1: \"))\n",
    "hidden2 = int(input(\"Enter number of neurons in Hidden Layer 2: \"))\n",
    "lr = float(input(\"Enter learning rate (e.g., 0.1): \"))\n",
    "epochs = int(input(\"Enter number of training epochs: \"))\n",
    "\n",
    "# ---- Training ----\n",
    "predicted_output, W1, W2, W3 = train_network(inputs, expected_outputs,hidden1_neurons=hidden1 ,hidden2_neurons=hidden2,lr=lr,epochs=epochs)\n",
    "# ---- Results ----\n",
    "print(\"\\nFinal predictions vs Expected:\")\n",
    "for i in range(len(inputs)):\n",
    "    print(f\"Input: {inputs[i]} \\t Expected: {expected_outputs[i][0]} \\t Predicted: {predicted_output[i][0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
