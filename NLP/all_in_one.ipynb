{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a089a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#22. Text Preprocessing – Cleaning, Lowercase, Tokenization, Stopwords, Spelling Correction\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "with open(\"text.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = re.sub(r'[^a-zA-Zs]', '', text)\n",
    "text = re.sub(r's+', ' ', text)\n",
    "text = text.lower()\n",
    "tokens = word_tokenize(text)\n",
    "filtered = [w for w in tokens if w not in stopwords.words('english')]\n",
    "corrected = [str(TextBlob(word).correct()) for word in filtered]\n",
    "\n",
    "print(corrected)\n",
    "\n",
    "\n",
    "#23. Text Preprocessing – Cleaning, Lowercase, Stemming, Lemmatization, 3-Word Phrases\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "with open(\"text.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = re.sub(r'[^a-zA-Zs]', '', text)\n",
    "text = re.sub(r's+', ' ', text)\n",
    "text = text.lower()\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "stemmed = [stemmer.stem(w) for w in tokens]\n",
    "lemmatized = [lemmatizer.lemmatize(w) for w in stemmed]\n",
    "\n",
    "three_grams = [' '.join(lemmatized[i:i+3]) for i in range(len(lemmatized)-2)]\n",
    "print(three_grams)\n",
    "\n",
    "\n",
    "#24. NLP – One-Hot Encoding from 3 Text Files\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "corpus = \"\"\n",
    "for filename in [\"file1.txt\", \"file2.txt\", \"file3.txt\"]:\n",
    "    with open(filename, \"r\") as file:\n",
    "        corpus += file.read() + \" \"\n",
    "\n",
    "words = list(set(re.sub(r'[^a-zA-Zs]', '', corpus.lower()).split()))\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded = encoder.fit_transform([[w] for w in words])\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"One-hot Encoding:\")\n",
    "print(encoded)\n",
    "\n",
    "\n",
    "#25. NLP – Bag of Words for Movie Reviews (3 Text Files)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = []\n",
    "for file_name in [\"review1.txt\", \"review2.txt\", \"review3.txt\"]:\n",
    "    with open(file_name, \"r\") as f:\n",
    "        documents.append(f.read())\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Bag of Words Matrix:\")\n",
    "print(X.toarray())\n",
    "print(\"Vocabulary:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "#26. NLP – TF-IDF for Tourist Place Descriptions (3 Text Files)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = []\n",
    "for file_name in [\"place1.txt\", \"place2.txt\", \"place3.txt\"]:\n",
    "    with open(file_name, \"r\") as f:\n",
    "        documents.append(f.read())\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(X.toarray())\n",
    "print(\"Vocabulary:\")\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
